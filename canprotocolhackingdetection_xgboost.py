# -*- coding: utf-8 -*-
"""CANProtocolHackingDetection_XGBoost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AqRx6uBTgKqvFBhASV3a9X7FPDxAiw05
"""

import pandas as pd
import numpy as np

def load_dataset(path):
    df = pd.read_csv(path)

    assert df.isna().any().any() == False, 'There is at least one missing value.'
    assert df['Timestamp'].is_monotonic_increasing, 'Timestamp is not sorted.'

    df['abstime'] = pd.to_datetime(df['Timestamp'], unit='s').round('us')
    df['monotime'] = df['Timestamp'] - df['Timestamp'].min()
    df['aid_int'] = df['Arbitration_ID'].map(lambda x: int(x,16))
    df['y'] = df['Class'].map({'Normal':0, 'Attack': 1})
    df['time_interval'] = df.groupby('Arbitration_ID')['Timestamp'].diff()
    df['time_interval_near'] = df['Timestamp'].diff()
    df['dt_squared'] = df['time_interval_near'].diff()

    return df

df_train = load_dataset('./Pre_train_D_2.csv')
df_test = load_dataset('./Pre_submit_D.csv')
df_train

def get_H(series_aid):
    count = series_aid.value_counts()
    p_i = count / series_aid.shape[0]
    return - (p_i * np.log(p_i)).sum()

df_train['entropy'] = df_train.rolling(window=2402, min_periods=2402, step=10)['aid_int'].apply(get_H)
df_train['entropy'] = df_train['entropy'].ffill()
df_test['entropy'] = df_test.rolling(window=2402, min_periods=2402, step=10)['aid_int'].apply(get_H)
df_test['entropy'] = df_test['entropy'].ffill()

df_train_except_nan = df_train[(~df_train['time_interval'].isna()) & (~df_train['entropy'].isna()) & (~df_train['dt_squared'].isna())]
df_test_except_nan = df_test[(~df_test['time_interval'].isna()) & (~df_test['entropy'].isna()) & (~df_test['dt_squared'].isna())]
df_train_except_nan

data_train = df_train_except_nan[['time_interval', 'entropy', 'dt_squared']]
target_train = df_train_except_nan['y']
data_test = df_test_except_nan[['time_interval', 'entropy', 'dt_squared']]
target_test = df_test_except_nan['y']

def Training_XGBoost(model, data_train, target_train):
  model.fit(data_train, target_train)
  return model

import xgboost as xgb
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import *

xgb_model = xgb.XGBClassifier(n_estimators = 100, learning_rate = 0.1, min_child_weight = 6, max_depth = 10)
model_XGBoost = Training_XGBoost(xgb_model, data_train, target_train)

Prediction = model_XGBoost.predict(data_test)
Prediction = np.where(Prediction > 0.5, 1, 0)

print("{}:".format('XGBoost Confusion matrix: \n'), confusion_matrix(target_test, Prediction))
print("{}:".format('XGBoost Accuracy: \n'), accuracy_score(target_test, Prediction))
print("{}:".format('XGBoost Precision: \n'), precision_score(target_test, Prediction))
print("{}:".format('XGBoost Recall: \n'), recall_score(target_test, Prediction))
print("{}:".format('XGBoost F1 score: \n'), f1_score(target_test, Prediction))

import xgboost as xgb
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import *

xgb_model = xgb.XGBClassifier()

param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.05, 0.1, 0.2],
    "min_child_weight" : [1, 3, 6, 10],
    'max_depth': [5, 7, 10]
}

grid_search = GridSearchCV(xgb_model, param_grid, scoring='f1')
grid_search.fit(data_train, target_train)

best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

best_model = grid_search.best_estimator_

model_XGBoost = Training_XGBoost(best_model, data_train, target_train)
Prediction = best_model.predict(data_test)
Prediction = np.where(Prediction > 0.5, 1, 0)

print("{}:".format('XGBoost Confusion matrix: \n'), confusion_matrix(target_test, Prediction))
print("{}:".format('XGBoost Accuracy: \n'), accuracy_score(target_test, Prediction))
print("{}:".format('XGBoost Precision: \n'), precision_score(target_test, Prediction))
print("{}:".format('XGBoost Recall: \n'), recall_score(target_test, Prediction))
print("{}:".format('XGBoost F1 score: \n'), f1_score(target_test, Prediction))